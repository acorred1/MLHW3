\documentclass{article}
\usepackage{amssymb,amsmath, verbatim}
%\usepackage{titlesec}  
\usepackage{epsfig}
%\usepackage{placeins}
\usepackage[margin=2cm]{geometry}
%\usepackage{fancyhdr}
%\pagestyle{fancy}
%\renewcommand{\headrulewidth}{0pt}
%\renewcommand{\footrulewidth}{1pt}

\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage{listings}
\usepackage{bm}

%\usepackage{names}

\DeclareMathOperator*{\argmax}{\arg\!\max}
\DeclareMathOperator*{\argmin}{\arg\!\min}

\newcommand{\pdiff}[2]{
        \frac{\partial #1}{\partial #2}
}

\newcommand{\diff}[2]{
        \frac{d #1}{d #2}
}


% Footer
%\lfoot{\thepage}
%\rfoot{\vspace{-.8cm} Nick Grunloh, Andrea Corredor, Abhinav Venkateswar Venkataraman
%\\\vspace{.14cm}
%\footnotesize grunloh@soe.ucsc.edu, acorredo@ucsc.edu, abhinav@soe.ucsc.eduu}


                 
%\titleformat{\subsection}[runin]{}{}{}{}[] % makes subsections not appear on their own lines


%Header
\newcommand{\header}[5]{
        \begin{minipage}[h!]{0.63\textwidth}
                \centering
                { \LARGE \textbf{ \textsc{#1} } }
        \end{minipage}
        \begin{minipage}[h!]{0.37\textwidth}
                \centering
                {#2}\\
                {#3}\\
        \end{minipage}
}

\begin{document}

\header{Machine Learning HW \#3}
       {Nick Grunloh\\Andrea Corredor\\Abhinav Venkateswar Venkataraman}
       {11/5/2013}
\\\\


\section{Linear Regression}

\subsection*{Training Set}
Running Weka's Linear Regression on the ful training set we obtain the model below:

 \[ t =    -0.1343 * x_{1} +  1.8477 * x_{2} + -0.8966 * x_{3} + 4.3608 \]

\noindent And a root mean squared error of $0.1897$. 

\subsection*{Predicting $\hat{t}$}

\begin{align*}
 x = [3,3,5]  \\
 \hat{t} = -0.1343 * 3 +  1.8477 * 3 + -0.8966 * 5 + 4.3608  \\
\hat{t} = 5.018 
\end{align*}

\subsection*{Comparing weights}
Computing the weights using Bishop (3.34) $ w = (X^{T}X)^{-1}X^{T}t$  we obtain,

\[w = \begin{bmatrix}
       4.3608029  \\[0.3em]
       -0.1342938  \\[0.3em]
       1.8476838 \\[0.3em]
       -0.8965848
     \end{bmatrix} \]

These weights are essentially identical to the ones computed by Weka's Linear Regression, except that Weka's have been rounded to $4^{th}$ decimal place.

\subsection*{Permutting the rows of $X$}
Reordering the examples has no impact on the weights computed with Bishop (3.34)
\[w_{reordered} = \begin{bmatrix}
       4.3608029  \\[0.3em]
       -0.1342938  \\[0.3em]
       1.8476838 \\[0.3em]
       -0.8965848
     \end{bmatrix} \]

It also did not affect the result of Weka's Linear Regression, which remained exactly the same as those of \mbox{part a}. 

\section{Interaction of sample size with the prior}

\section{Logistic Regression}
\begin{align*}
p(1|\bm{x},~\bm{w}) &= \frac{ \exp\left\{ \bm{x}'\bm{w} \right\} }{ 1+\exp\left\{ \bm{x}'\bm{w} \right\} }\\
p(0|\bm{x},~\bm{w}) &= 1-p(1|\bm{x},~\bm{w})\\
		    &= \left(\frac{1+ \exp\left\{ \bm{x}'\bm{w} \right\} }{ 1+\exp\left\{ \bm{x}'\bm{w} \right\} }\right)-\left(\frac{ \exp\left\{ \bm{x}'\bm{w} \right\} }{ 1+\exp\left\{ \bm{x}'\bm{w} \right\} }\right)\\
		    &= \frac{ 1 }{ 1+\exp\left\{ \bm{x}'\bm{w} \right\} }\\
		    &\nonumber\\
\ln\left\{ \frac{p(1|\bm{x},~\bm{w})}{p(0|\bm{x},~\bm{w})} \right\} &= \ln \left\{ 
	\frac{ \frac{ \exp\left\{ \bm{x}'\bm{w} \right\} }{ 1+\exp\left\{ \bm{x}'\bm{w} \right\} } }
	     { \frac{ 1 }{ 1+\exp\left\{ \bm{x}'\bm{w} \right\} } } 
	\right\}\\
		    &= \ln\left\{ \exp\left\{ \bm{x}'\bm{w} \right\} \right\}\\
		    &= \bm{x}'\bm{w}~=~\bm{w}'\bm{x}
\end{align*}

\section{Gradient of the cross-entropy error}
\begin{align*}
y_n &= \sigma(\bm{x}_{i}^{T}\bm{w})=\frac{\exp\left\{ a \right\}}{\left( 1+\exp\left\{ a\right\} \right)}\\
&\nonumber\\
\diff{\sigma(a)}{a} &= \diff{\exp\left\{ a \right\} \left( 1+\exp\left\{ a\right\} \right)^{-1}}{a}\\
		    &= \frac{\exp\left\{ a \right\}}{\left( 1+\exp\left\{ a\right\} \right)} -
		       \left( 
			\frac{\exp\left\{ a \right\}}{\left( 1+\exp\left\{ a\right\} \right)}
		       \right)
		       \left(
			\frac{\exp\left\{ a \right\}}{\left( 1+\exp\left\{ a\right\} \right)}
		       \right)\\
		    &= \sigma(a)\left(1-\sigma(a)\right)\\
&\nonumber\\
E(\bm{w}) &= -\sum_{i=1}^N t_i \ln\left\{ \sigma(\bm{x}_{i}^{T} \bm{w}) \right\} + (1-t_i)\ln\left\{1-\sigma(\bm{x}_{i}^{T}\bm{w})\right\}\\
\nabla_{\bm{w}} E(\bm{w}) &= -\nabla_{\bm{w}}\left[\sum_{i=1}^N t_i \ln\left\{ \sigma(\bm{x}_{i}^{T}\bm{w}) \right\} + (1-t_i)\ln\left\{1-\sigma(\bm{x}_{i}^{T}\bm{w})\right\}\right]\\
&=-\sum_{i=1}^N t_i \frac{1}{\sigma} \sigma(1-\sigma)\bm{x}_{i} + (1-t_i) \frac{1}{1-\sigma} \left(-\sigma(1-\sigma)\bm{x}_{i}\right)\\
&=\sum_{i=1}^{N} -t_i \bm{x}_{i} + t_i \sigma \bm{x}_{i} + \sigma \bm{x}_{i} - t_i \sigma \bm{x}_{i}\\
&=\sum_{i=1}^{N} (\sigma(\bm{x}_{i}^{T}\bm{w}) - t_i)\bm{x}_{i}
\end{align*}
\section{Perceptron algorithm with noise experiment}

\subsection*{Part a}
Epochs = 2
Prediction errors = 6

\subsection*{Part b}

Epochs = 2
Prediction errors = 41

Would you expect it to take more or fewer epochs than part a? Why?
%I don't see why it would raise it. Does this have something to do with the gap?
We expect it to take the same number of epochs, because the probability of being labeled plus is about the same as of being labeled minus (?). Quiver (?)


\subsection*{Part c}

Which of these will be more accurate on new unseen data? Create a separate test set (with the random noise) and evaluate the accuracy of these three hypotheses on it. Ambitious students may run this experiment multiple times (say 10)and report the average accuracies.
\\
Running the experiment 10 times: \\
Avg. accuracy for $w_{1000}$ = $0.7706$ \\
Avg. accuracy for $w_{avg}$ = $0.8384$ \\
Avg. accuracy for $w_{vote}$ = $0.8360$ \\

\noindent $w_{avg}$ and $w_{vote}$ are consistently superior to $w_{1000}$, with $w_{avg}$ slightly superior to $w_{vote}$.

\clearpage
\section{Appendix}

\begin{tiny}

\verbatiminput{./perceptron.R}

\end{tiny}

\end{document}
